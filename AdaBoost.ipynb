{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KYCg6Kre-lN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import scipy.io as sio\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTSFwlZelM46"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import copy\n",
        "\n",
        "class Adaboost:\n",
        "    \"\"\"\n",
        "    D : Dictionnary of weights D[i] = list of m weights\n",
        "    \"\"\"\n",
        "    D = np.empty(0)\n",
        "\n",
        "    \"\"\"\n",
        "    alpha_list : List of alphas of the Adaboost best h\n",
        "    \"\"\"\n",
        "    alpha_list = np.empty(0)\n",
        "\n",
        "    \"\"\"\n",
        "    h_list : List of h_t best classifiers selected\n",
        "    \"\"\"\n",
        "    h_list = np.empty(0)\n",
        "\n",
        "    \"\"\"\n",
        "    T : number of iterations\n",
        "    \"\"\"\n",
        "    T = 0\n",
        "\n",
        "    \"\"\"\n",
        "    decisionTreeClassifier : Decision tree classifier\n",
        "    \"\"\"\n",
        "    decisionTreeClassifier = 0\n",
        "\n",
        "    \"\"\"\n",
        "    regularization_power :  None if no regularization else a positive integer.\n",
        "    -> Regularize with a (Sum(alpha_t D_t(i)) power regularization)\n",
        "    \"\"\"\n",
        "    regularization_power = None\n",
        "\n",
        "    \"\"\"\n",
        "    regularization_C :  None if no regularization else a positive integer.\n",
        "    -> Regularize with a (Sum(alpha_t D_t(i)) power regularization)\n",
        "    \"\"\"\n",
        "    regularization_C = 1\n",
        "\n",
        "    \"\"\"\n",
        "    Version : We try multiple regularizations\n",
        "    - version 0 : base regularization : Weight update -> C * (Psi_t-1(i) - Psi_t(i))\n",
        "    - version 1 : Updated : Weight update -> - C * Psi_t(i)\n",
        "    \"\"\"\n",
        "    version = 0\n",
        "\n",
        "    \"\"\"\n",
        "    Random : Randomize weights at the beginning\n",
        "    \"\"\"\n",
        "    random = False\n",
        "\n",
        "    def __init__(self, T, regularization_power=None, regularization_C=None, version=0, random=False):\n",
        "        self.version = version\n",
        "        self.regularization_power = regularization_power\n",
        "        self.regularization_C = regularization_C\n",
        "        self.D = np.empty(0)\n",
        "        self.alphas = np.empty(0)\n",
        "        self.h_t = np.empty(0)\n",
        "        self.T = T\n",
        "        self.random = random\n",
        "        self.decisionTreeClassifier = DecisionTreeClassifier(criterion=\"entropy\", max_depth=1, presort=True)\n",
        "\n",
        "    def fit(self, trainFeatures, trainLabels):\n",
        "        error_rate = []\n",
        "        feat = []\n",
        "\n",
        "        \"\"\"\n",
        "        fit Adaboost to the training set, with T iterations.\n",
        "        \"\"\"\n",
        "\n",
        "        m, n = trainFeatures.shape\n",
        "\n",
        "        self.D = np.ndarray(shape=(self.T + 1, m))\n",
        "\n",
        "        if self.random:\n",
        "            self.D.fill( 1.0 / m)\n",
        "            self.D[0] = np.random.rand(m)\n",
        "            self.D[0] /= self.D[0].sum()\n",
        "        else:\n",
        "            self.D.fill(1.0 / m)\n",
        "\n",
        "        psi_t = np.ndarray(shape=(self.T, m))\n",
        "\n",
        "        for t in range(self.T):\n",
        "            h_t = self.decisionTreeClassifier.fit(trainFeatures, trainLabels, self.D[t])\n",
        "\n",
        "            h_t_predict_all = h_t.predict(trainFeatures)\n",
        "            epsilon_t =  np.where(h_t_predict_all != trainLabels, 1.0, 0) * self.D[t]\n",
        "            epsilon_t = epsilon_t.sum()\n",
        "\n",
        "            alpha_t = np.log((1 - epsilon_t) / epsilon_t) / 2\n",
        "\n",
        "            if epsilon_t == 0:\n",
        "                alpha_t = 1\n",
        "\n",
        "            self.alpha_list = np.append(self.alpha_list, alpha_t)\n",
        "\n",
        "            regularization_vector = np.zeros(m)\n",
        "\n",
        "            if self.regularization_power != None:\n",
        "                regularization_vector -=  np.power(np.transpose(self.D[:t+1]).dot(self.alpha_list[:t+1] / np.linalg.norm(self.alpha_list[:t+1])), self.regularization_power)\n",
        "                psi_t[t] = - regularization_vector\n",
        "                if t > 0 and self.version == 0:\n",
        "                    regularization_vector += np.power(np.transpose(self.D[:t]).dot(self.alpha_list[:t]/ np.linalg.norm(self.alpha_list[:t])), self.regularization_power)\n",
        "                regularization_vector *= self.regularization_C\n",
        "\n",
        "            self.D[t + 1] =  self.D[t] * np.exp(- alpha_t * trainLabels * h_t_predict_all + regularization_vector)\n",
        "\n",
        "            if self.regularization_power != None:\n",
        "                self.D[t + 1] /= np.linalg.norm(self.D[t + 1], ord=1)\n",
        "            else:\n",
        "                Z_t = 2 * np.sqrt(epsilon_t * ( 1 - epsilon_t))\n",
        "                self.D[t + 1] /= Z_t\n",
        "\n",
        "            if len(self.alpha_list) > 1 and self.alpha_list[-2] == alpha_t:\n",
        "                print (\" Stoping the training at t = {0}, convergence.\".format(t))\n",
        "                break\n",
        "\n",
        "            error_rate += [epsilon_t]\n",
        "            feat += [np.argmax(h_t.feature_importances_)]\n",
        "\n",
        "            self.h_list = np.append(self.h_list, copy.copy(h_t))\n",
        "            if epsilon_t == 0:\n",
        "                break\n",
        "\n",
        "        return self.D\n",
        "\n",
        "    def predict_proba(self, x, T=0):\n",
        "        \"\"\"\n",
        "        Return the g(x), not sgn(g(x))\n",
        "        \"\"\"\n",
        "        if T == 0 or T > len(self.alpha_list):\n",
        "            T = len(self.alpha_list)\n",
        "\n",
        "        scores = np.zeros(x.shape[0])\n",
        "        for i in range(T):\n",
        "            scores = scores + self.alpha_list[i] * self.h_list[i].predict(x)\n",
        "        return scores\n",
        "\n",
        "    def predict(self, x, T=0):\n",
        "        \"\"\"\n",
        "        Once trained, predict a label given x\n",
        "        T : if we compute adaboost(1000) and we want adaboost(10), it's the 10 first terms of adaboost(1000). Let's use it.\n",
        "        \"\"\"\n",
        "        if T == 0 or T > len(self.h_list):\n",
        "            T = len(self.h_list)\n",
        "\n",
        "        scores = np.zeros(x.shape[0])\n",
        "\n",
        "        for i in range(T):\n",
        "            scores = scores + self.alpha_list[i] * self.h_list[i].predict(x)\n",
        "        return np.where(scores > 0, 1, -1)\n",
        "\n",
        "    def error(self, x, y, T):\n",
        "        \"\"\"\n",
        "        Return the percentage of errors in the prediction.\n",
        "        \"\"\"\n",
        "        return np.where(self.predict(x, T) * y == 1, 0, 1).sum() * 1.0 / x.shape[0]\n",
        "\n",
        "    def score(self, x, y, T = None):\n",
        "        if T == None or T > self.T:\n",
        "            T = self.T\n",
        "        return np.where(self.predict(x, T) * y == 1, 1, 0).sum() * 1.0 / x.shape[0]\n",
        "\n",
        "    def toString(self):\n",
        "        print (\"T : {0}, Alphas : {0}\".format(self.T, len(self.alpha_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDkW_r5ZfbQo",
        "outputId": "662a8593-ea6d-4268-bc6d-b3c26dd2ded2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03BE3bT3fkHg"
      },
      "source": [
        "def loadData():\n",
        "  data_path = os.path.join('gdrive/My Drive/HIS_Final','data')\n",
        "  data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "  labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "  return data,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCFYt29tfo-G"
      },
      "source": [
        "test_ratio = 0.2\n",
        "windowSize = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loxdaXmUfr8I"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "def extract_pixels(dataset, ground_truth):\n",
        "    df = pd.DataFrame()\n",
        "    for i in tqdm(range(dataset.shape[2])):\n",
        "        df = pd.concat([df, pd.DataFrame(dataset[:, :, i].ravel())], axis=1)\n",
        "    df = pd.concat([df, pd.DataFrame(ground_truth.ravel())], axis=1)\n",
        "    df.columns = [f'band-{i}' for i in range(1, 1+dataset.shape[2])]+['class']\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "pRuFRhcyfuje",
        "outputId": "2956eee8-2ce8-4f9c-c907-8afe616e13ce"
      },
      "source": [
        "dataset, ground_truth = loadData()\n",
        "df = extract_pixels(dataset, ground_truth)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 257.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>band-1</th>\n",
              "      <th>band-2</th>\n",
              "      <th>band-3</th>\n",
              "      <th>band-4</th>\n",
              "      <th>band-5</th>\n",
              "      <th>band-6</th>\n",
              "      <th>band-7</th>\n",
              "      <th>band-8</th>\n",
              "      <th>band-9</th>\n",
              "      <th>band-10</th>\n",
              "      <th>band-11</th>\n",
              "      <th>band-12</th>\n",
              "      <th>band-13</th>\n",
              "      <th>band-14</th>\n",
              "      <th>band-15</th>\n",
              "      <th>band-16</th>\n",
              "      <th>band-17</th>\n",
              "      <th>band-18</th>\n",
              "      <th>band-19</th>\n",
              "      <th>band-20</th>\n",
              "      <th>band-21</th>\n",
              "      <th>band-22</th>\n",
              "      <th>band-23</th>\n",
              "      <th>band-24</th>\n",
              "      <th>band-25</th>\n",
              "      <th>band-26</th>\n",
              "      <th>band-27</th>\n",
              "      <th>band-28</th>\n",
              "      <th>band-29</th>\n",
              "      <th>band-30</th>\n",
              "      <th>band-31</th>\n",
              "      <th>band-32</th>\n",
              "      <th>band-33</th>\n",
              "      <th>band-34</th>\n",
              "      <th>band-35</th>\n",
              "      <th>band-36</th>\n",
              "      <th>band-37</th>\n",
              "      <th>band-38</th>\n",
              "      <th>band-39</th>\n",
              "      <th>band-40</th>\n",
              "      <th>...</th>\n",
              "      <th>band-162</th>\n",
              "      <th>band-163</th>\n",
              "      <th>band-164</th>\n",
              "      <th>band-165</th>\n",
              "      <th>band-166</th>\n",
              "      <th>band-167</th>\n",
              "      <th>band-168</th>\n",
              "      <th>band-169</th>\n",
              "      <th>band-170</th>\n",
              "      <th>band-171</th>\n",
              "      <th>band-172</th>\n",
              "      <th>band-173</th>\n",
              "      <th>band-174</th>\n",
              "      <th>band-175</th>\n",
              "      <th>band-176</th>\n",
              "      <th>band-177</th>\n",
              "      <th>band-178</th>\n",
              "      <th>band-179</th>\n",
              "      <th>band-180</th>\n",
              "      <th>band-181</th>\n",
              "      <th>band-182</th>\n",
              "      <th>band-183</th>\n",
              "      <th>band-184</th>\n",
              "      <th>band-185</th>\n",
              "      <th>band-186</th>\n",
              "      <th>band-187</th>\n",
              "      <th>band-188</th>\n",
              "      <th>band-189</th>\n",
              "      <th>band-190</th>\n",
              "      <th>band-191</th>\n",
              "      <th>band-192</th>\n",
              "      <th>band-193</th>\n",
              "      <th>band-194</th>\n",
              "      <th>band-195</th>\n",
              "      <th>band-196</th>\n",
              "      <th>band-197</th>\n",
              "      <th>band-198</th>\n",
              "      <th>band-199</th>\n",
              "      <th>band-200</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3172</td>\n",
              "      <td>4142</td>\n",
              "      <td>4506</td>\n",
              "      <td>4279</td>\n",
              "      <td>4782</td>\n",
              "      <td>5048</td>\n",
              "      <td>5213</td>\n",
              "      <td>5106</td>\n",
              "      <td>5053</td>\n",
              "      <td>4750</td>\n",
              "      <td>4816</td>\n",
              "      <td>4769</td>\n",
              "      <td>4610</td>\n",
              "      <td>4805</td>\n",
              "      <td>4828</td>\n",
              "      <td>4861</td>\n",
              "      <td>4767</td>\n",
              "      <td>4624</td>\n",
              "      <td>4549</td>\n",
              "      <td>4463</td>\n",
              "      <td>4462</td>\n",
              "      <td>4446</td>\n",
              "      <td>4445</td>\n",
              "      <td>4336</td>\n",
              "      <td>4381</td>\n",
              "      <td>4319</td>\n",
              "      <td>4207</td>\n",
              "      <td>4305</td>\n",
              "      <td>4311</td>\n",
              "      <td>3991</td>\n",
              "      <td>4168</td>\n",
              "      <td>3942</td>\n",
              "      <td>4061</td>\n",
              "      <td>4362</td>\n",
              "      <td>4318</td>\n",
              "      <td>4252</td>\n",
              "      <td>4869</td>\n",
              "      <td>5284</td>\n",
              "      <td>5055</td>\n",
              "      <td>3591</td>\n",
              "      <td>...</td>\n",
              "      <td>1396</td>\n",
              "      <td>1381</td>\n",
              "      <td>1396</td>\n",
              "      <td>1381</td>\n",
              "      <td>1353</td>\n",
              "      <td>1346</td>\n",
              "      <td>1341</td>\n",
              "      <td>1332</td>\n",
              "      <td>1324</td>\n",
              "      <td>1310</td>\n",
              "      <td>1318</td>\n",
              "      <td>1330</td>\n",
              "      <td>1310</td>\n",
              "      <td>1292</td>\n",
              "      <td>1280</td>\n",
              "      <td>1275</td>\n",
              "      <td>1266</td>\n",
              "      <td>1264</td>\n",
              "      <td>1233</td>\n",
              "      <td>1241</td>\n",
              "      <td>1232</td>\n",
              "      <td>1215</td>\n",
              "      <td>1215</td>\n",
              "      <td>1187</td>\n",
              "      <td>1168</td>\n",
              "      <td>1171</td>\n",
              "      <td>1150</td>\n",
              "      <td>1134</td>\n",
              "      <td>1123</td>\n",
              "      <td>1135</td>\n",
              "      <td>1094</td>\n",
              "      <td>1090</td>\n",
              "      <td>1112</td>\n",
              "      <td>1090</td>\n",
              "      <td>1062</td>\n",
              "      <td>1069</td>\n",
              "      <td>1057</td>\n",
              "      <td>1020</td>\n",
              "      <td>1020</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2580</td>\n",
              "      <td>4266</td>\n",
              "      <td>4502</td>\n",
              "      <td>4426</td>\n",
              "      <td>4853</td>\n",
              "      <td>5249</td>\n",
              "      <td>5352</td>\n",
              "      <td>5353</td>\n",
              "      <td>5347</td>\n",
              "      <td>5065</td>\n",
              "      <td>5141</td>\n",
              "      <td>5100</td>\n",
              "      <td>4994</td>\n",
              "      <td>5172</td>\n",
              "      <td>5290</td>\n",
              "      <td>5289</td>\n",
              "      <td>5217</td>\n",
              "      <td>5053</td>\n",
              "      <td>5033</td>\n",
              "      <td>4939</td>\n",
              "      <td>4931</td>\n",
              "      <td>4941</td>\n",
              "      <td>4902</td>\n",
              "      <td>4824</td>\n",
              "      <td>4859</td>\n",
              "      <td>4805</td>\n",
              "      <td>4698</td>\n",
              "      <td>4794</td>\n",
              "      <td>4806</td>\n",
              "      <td>4452</td>\n",
              "      <td>4628</td>\n",
              "      <td>4433</td>\n",
              "      <td>4643</td>\n",
              "      <td>4967</td>\n",
              "      <td>4853</td>\n",
              "      <td>4760</td>\n",
              "      <td>5449</td>\n",
              "      <td>5768</td>\n",
              "      <td>5684</td>\n",
              "      <td>3987</td>\n",
              "      <td>...</td>\n",
              "      <td>1421</td>\n",
              "      <td>1415</td>\n",
              "      <td>1428</td>\n",
              "      <td>1415</td>\n",
              "      <td>1379</td>\n",
              "      <td>1370</td>\n",
              "      <td>1360</td>\n",
              "      <td>1353</td>\n",
              "      <td>1352</td>\n",
              "      <td>1336</td>\n",
              "      <td>1346</td>\n",
              "      <td>1351</td>\n",
              "      <td>1330</td>\n",
              "      <td>1315</td>\n",
              "      <td>1305</td>\n",
              "      <td>1292</td>\n",
              "      <td>1282</td>\n",
              "      <td>1286</td>\n",
              "      <td>1259</td>\n",
              "      <td>1259</td>\n",
              "      <td>1250</td>\n",
              "      <td>1229</td>\n",
              "      <td>1232</td>\n",
              "      <td>1195</td>\n",
              "      <td>1177</td>\n",
              "      <td>1184</td>\n",
              "      <td>1153</td>\n",
              "      <td>1137</td>\n",
              "      <td>1138</td>\n",
              "      <td>1137</td>\n",
              "      <td>1108</td>\n",
              "      <td>1104</td>\n",
              "      <td>1117</td>\n",
              "      <td>1091</td>\n",
              "      <td>1079</td>\n",
              "      <td>1085</td>\n",
              "      <td>1064</td>\n",
              "      <td>1029</td>\n",
              "      <td>1020</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3687</td>\n",
              "      <td>4266</td>\n",
              "      <td>4421</td>\n",
              "      <td>4498</td>\n",
              "      <td>5019</td>\n",
              "      <td>5293</td>\n",
              "      <td>5438</td>\n",
              "      <td>5427</td>\n",
              "      <td>5383</td>\n",
              "      <td>5132</td>\n",
              "      <td>5227</td>\n",
              "      <td>5172</td>\n",
              "      <td>5097</td>\n",
              "      <td>5313</td>\n",
              "      <td>5411</td>\n",
              "      <td>5412</td>\n",
              "      <td>5341</td>\n",
              "      <td>5191</td>\n",
              "      <td>5140</td>\n",
              "      <td>5069</td>\n",
              "      <td>5110</td>\n",
              "      <td>5119</td>\n",
              "      <td>5046</td>\n",
              "      <td>4981</td>\n",
              "      <td>5023</td>\n",
              "      <td>4987</td>\n",
              "      <td>4862</td>\n",
              "      <td>4965</td>\n",
              "      <td>4992</td>\n",
              "      <td>4595</td>\n",
              "      <td>4756</td>\n",
              "      <td>4529</td>\n",
              "      <td>4801</td>\n",
              "      <td>5077</td>\n",
              "      <td>4983</td>\n",
              "      <td>4868</td>\n",
              "      <td>5515</td>\n",
              "      <td>5972</td>\n",
              "      <td>5913</td>\n",
              "      <td>4027</td>\n",
              "      <td>...</td>\n",
              "      <td>1446</td>\n",
              "      <td>1440</td>\n",
              "      <td>1443</td>\n",
              "      <td>1425</td>\n",
              "      <td>1390</td>\n",
              "      <td>1379</td>\n",
              "      <td>1376</td>\n",
              "      <td>1363</td>\n",
              "      <td>1355</td>\n",
              "      <td>1347</td>\n",
              "      <td>1361</td>\n",
              "      <td>1356</td>\n",
              "      <td>1341</td>\n",
              "      <td>1330</td>\n",
              "      <td>1321</td>\n",
              "      <td>1304</td>\n",
              "      <td>1290</td>\n",
              "      <td>1289</td>\n",
              "      <td>1263</td>\n",
              "      <td>1269</td>\n",
              "      <td>1261</td>\n",
              "      <td>1245</td>\n",
              "      <td>1241</td>\n",
              "      <td>1214</td>\n",
              "      <td>1185</td>\n",
              "      <td>1188</td>\n",
              "      <td>1156</td>\n",
              "      <td>1147</td>\n",
              "      <td>1149</td>\n",
              "      <td>1144</td>\n",
              "      <td>1111</td>\n",
              "      <td>1114</td>\n",
              "      <td>1114</td>\n",
              "      <td>1100</td>\n",
              "      <td>1065</td>\n",
              "      <td>1092</td>\n",
              "      <td>1061</td>\n",
              "      <td>1030</td>\n",
              "      <td>1016</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2749</td>\n",
              "      <td>4258</td>\n",
              "      <td>4603</td>\n",
              "      <td>4493</td>\n",
              "      <td>4958</td>\n",
              "      <td>5234</td>\n",
              "      <td>5417</td>\n",
              "      <td>5355</td>\n",
              "      <td>5349</td>\n",
              "      <td>5096</td>\n",
              "      <td>5147</td>\n",
              "      <td>5078</td>\n",
              "      <td>5040</td>\n",
              "      <td>5237</td>\n",
              "      <td>5321</td>\n",
              "      <td>5344</td>\n",
              "      <td>5255</td>\n",
              "      <td>5121</td>\n",
              "      <td>5035</td>\n",
              "      <td>4956</td>\n",
              "      <td>4994</td>\n",
              "      <td>4980</td>\n",
              "      <td>4905</td>\n",
              "      <td>4857</td>\n",
              "      <td>4900</td>\n",
              "      <td>4831</td>\n",
              "      <td>4720</td>\n",
              "      <td>4848</td>\n",
              "      <td>4847</td>\n",
              "      <td>4484</td>\n",
              "      <td>4613</td>\n",
              "      <td>4402</td>\n",
              "      <td>4674</td>\n",
              "      <td>4966</td>\n",
              "      <td>4848</td>\n",
              "      <td>4776</td>\n",
              "      <td>5473</td>\n",
              "      <td>5894</td>\n",
              "      <td>5789</td>\n",
              "      <td>4086</td>\n",
              "      <td>...</td>\n",
              "      <td>1432</td>\n",
              "      <td>1427</td>\n",
              "      <td>1426</td>\n",
              "      <td>1416</td>\n",
              "      <td>1386</td>\n",
              "      <td>1374</td>\n",
              "      <td>1375</td>\n",
              "      <td>1359</td>\n",
              "      <td>1343</td>\n",
              "      <td>1343</td>\n",
              "      <td>1354</td>\n",
              "      <td>1351</td>\n",
              "      <td>1333</td>\n",
              "      <td>1329</td>\n",
              "      <td>1313</td>\n",
              "      <td>1296</td>\n",
              "      <td>1280</td>\n",
              "      <td>1281</td>\n",
              "      <td>1251</td>\n",
              "      <td>1255</td>\n",
              "      <td>1253</td>\n",
              "      <td>1238</td>\n",
              "      <td>1223</td>\n",
              "      <td>1207</td>\n",
              "      <td>1188</td>\n",
              "      <td>1188</td>\n",
              "      <td>1154</td>\n",
              "      <td>1143</td>\n",
              "      <td>1144</td>\n",
              "      <td>1146</td>\n",
              "      <td>1122</td>\n",
              "      <td>1108</td>\n",
              "      <td>1109</td>\n",
              "      <td>1109</td>\n",
              "      <td>1071</td>\n",
              "      <td>1088</td>\n",
              "      <td>1060</td>\n",
              "      <td>1030</td>\n",
              "      <td>1006</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2746</td>\n",
              "      <td>4018</td>\n",
              "      <td>4675</td>\n",
              "      <td>4417</td>\n",
              "      <td>4886</td>\n",
              "      <td>5117</td>\n",
              "      <td>5215</td>\n",
              "      <td>5096</td>\n",
              "      <td>5098</td>\n",
              "      <td>4834</td>\n",
              "      <td>4853</td>\n",
              "      <td>4857</td>\n",
              "      <td>4734</td>\n",
              "      <td>4879</td>\n",
              "      <td>4976</td>\n",
              "      <td>4958</td>\n",
              "      <td>4885</td>\n",
              "      <td>4754</td>\n",
              "      <td>4647</td>\n",
              "      <td>4532</td>\n",
              "      <td>4586</td>\n",
              "      <td>4591</td>\n",
              "      <td>4492</td>\n",
              "      <td>4453</td>\n",
              "      <td>4497</td>\n",
              "      <td>4398</td>\n",
              "      <td>4297</td>\n",
              "      <td>4408</td>\n",
              "      <td>4401</td>\n",
              "      <td>4102</td>\n",
              "      <td>4227</td>\n",
              "      <td>4075</td>\n",
              "      <td>4264</td>\n",
              "      <td>4529</td>\n",
              "      <td>4490</td>\n",
              "      <td>4438</td>\n",
              "      <td>5001</td>\n",
              "      <td>5378</td>\n",
              "      <td>5321</td>\n",
              "      <td>3779</td>\n",
              "      <td>...</td>\n",
              "      <td>1401</td>\n",
              "      <td>1397</td>\n",
              "      <td>1395</td>\n",
              "      <td>1390</td>\n",
              "      <td>1368</td>\n",
              "      <td>1349</td>\n",
              "      <td>1354</td>\n",
              "      <td>1340</td>\n",
              "      <td>1330</td>\n",
              "      <td>1324</td>\n",
              "      <td>1336</td>\n",
              "      <td>1332</td>\n",
              "      <td>1320</td>\n",
              "      <td>1307</td>\n",
              "      <td>1287</td>\n",
              "      <td>1283</td>\n",
              "      <td>1267</td>\n",
              "      <td>1265</td>\n",
              "      <td>1239</td>\n",
              "      <td>1240</td>\n",
              "      <td>1239</td>\n",
              "      <td>1229</td>\n",
              "      <td>1212</td>\n",
              "      <td>1202</td>\n",
              "      <td>1178</td>\n",
              "      <td>1178</td>\n",
              "      <td>1143</td>\n",
              "      <td>1135</td>\n",
              "      <td>1138</td>\n",
              "      <td>1135</td>\n",
              "      <td>1110</td>\n",
              "      <td>1107</td>\n",
              "      <td>1112</td>\n",
              "      <td>1094</td>\n",
              "      <td>1072</td>\n",
              "      <td>1087</td>\n",
              "      <td>1052</td>\n",
              "      <td>1034</td>\n",
              "      <td>1019</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   band-1  band-2  band-3  band-4  ...  band-198  band-199  band-200  class\n",
              "0    3172    4142    4506    4279  ...      1057      1020      1020      3\n",
              "1    2580    4266    4502    4426  ...      1064      1029      1020      3\n",
              "2    3687    4266    4421    4498  ...      1061      1030      1016      3\n",
              "3    2749    4258    4603    4493  ...      1060      1030      1006      3\n",
              "4    2746    4018    4675    4417  ...      1052      1034      1019      3\n",
              "\n",
              "[5 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaa3VtgKf-vq",
        "outputId": "071e5c4e-d0e0-4099-d7ee-42ad17c209f8"
      },
      "source": [
        "X = df.iloc[:, :-1].values\n",
        "\n",
        "y = df.iloc[:, -1].values\n",
        "X.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21025, 200), (21025,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nsXNEwrfyw3"
      },
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuLend34oP_E",
        "outputId": "441df5f0-2fdb-4eae-a2a1-0c0a5a67acf8"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "kpca = PCA(n_components=50, whiten=True) \n",
        "#pca = PCA(n_components=numComponents, svd_solver='arpack')\n",
        "principalComponents = kpca.fit_transform(X)\n",
        "\n",
        "principalComponents.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21025, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykpPfxd8f58e",
        "outputId": "2ad0bddc-1576-4cbb-874e-93d1a3404451"
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(principalComponents, y, test_ratio)\n",
        "\n",
        "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16820, 50), (4205, 50), (16820,), (4205,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN8gH1GbgTTy"
      },
      "source": [
        "bdt_real = AdaBoostClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dGB1k_Ig6kg",
        "outputId": "8d2df43a-3d79-4496-ce3a-7de9251cc74d"
      },
      "source": [
        "bdt_real.fit(Xtrain, ytrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.5,\n",
              "                   n_estimators=100, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yval-7XyhpFQ"
      },
      "source": [
        "y_pred=bdt_real.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFDEZ99lluRx"
      },
      "source": [
        "my_adaboost = Adaboost(100, 2, 10, 0)\n",
        "my_adaboost.fit(Xtrain, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttycBWopi4Ur",
        "outputId": "00362a99-56d5-4294-8d76-3256364b5768"
      },
      "source": [
        "accuracy_score(y_pred, ytest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5089179548156956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}